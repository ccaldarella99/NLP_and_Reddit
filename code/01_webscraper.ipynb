{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brown-witness",
   "metadata": {},
   "source": [
    "# Webscraper (for reddit.com, project 3)\n",
    "(Example: [https://www.youtube.com/watch?v=AcrjEWsMi_E](https://www.youtube.com/watch?v=AcrjEWsMi_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "announced-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Updated: 2021-04-24, 00:14\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "automatic-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "BASE_URL = 'https://api.pushshift.io/reddit/search/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quality-progress",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_scraper(size, sub_red_1 = 'AMA', sub_red_2 = 'AskReddit', coms_only=False, before=1, debug=False):\n",
    "    \"\"\"\n",
    "    Calls Reddit API and grabs two subreddit's comments and submissions \n",
    "    Takes 'size' and subreddit' params and passes them to the API\n",
    "    By default, this grabs comments and submissions, resulting in 4 Data Frames\n",
    "    coms_only boolean ONLY grabs comments (2 Data Frames, no submissions)\n",
    "    Function reads in files 'before_coms.csv' and 'before_subm.csv' to get\n",
    "    the oldest 'created_utc' from the last run (so it can pickup where it left off)\n",
    "    which is passed to the params as the value for 'before'\n",
    "    \n",
    "    ARGUMENTS\n",
    "    size: Number of Submissions or comments to pull; API is limited to at most 100 of either per pull\n",
    "    sub_red_1 (default: 'AMA'): Subreddit you want to pull from\n",
    "    sub_red_2 (default: 'AskReddit'): Another Subreddit you want to pull from\n",
    "    coms_only (default: False): Pull Comments only (True), or pull Submissions too (True)\n",
    "    before (default: 1): before value; deprecated - this no longer does anything (now reads from file)\n",
    "    debug (default: False): Set to true to view some randomly inserted print commands (both to terminal and log)\n",
    "    \n",
    "    RETURN\n",
    "    Returns a list of DataFrames: 1 comment DataFrame from \n",
    "        each subreddit (max 100 each)\n",
    "    And, if 'coms_only' is set to False, additionaly returns \n",
    "        1 submissions dataaframer from each subreddit (max 100 each)\n",
    "    \"\"\"\n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(3)\n",
    "    log_to_file(f'  Begin Iteration of call_scraper')\n",
    "    print_special_line_to_log(3)\n",
    "    \n",
    "    c = 0\n",
    "    sub_reddit = [sub_red_1, sub_red_2]\n",
    "    b4 = []\n",
    "    list_df = [[],[],[],[]]\n",
    "    \n",
    "    if(coms_only):\n",
    "        submission_comment = ['comment']    # FOR COMS ONLY\n",
    "    else:\n",
    "        submission_comment = ['comment', 'submission']   # FOR SUB AND COMS\n",
    "    \n",
    "    # read in before.csv values as a list, and apply them \n",
    "    if(coms_only):\n",
    "        old_before_list = read_in_before_csv('before_coms.csv')   # FOR COMS ONLY\n",
    "    else:\n",
    "        ob4_coms = read_in_before_csv('before_coms.csv')   # FOR SUB AND COMS\n",
    "        ob4_subm = read_in_before_csv('before_subm.csv')\n",
    "        old_before_list = [ ob4_coms[0], ob4_coms[1], ob4_subm[0], ob4_subm[1] ]\n",
    "    \n",
    "    for sc in submission_comment:\n",
    "        for sr in sub_reddit:\n",
    "            before = 0\n",
    "            if(c < len(old_before_list)):\n",
    "                before = old_before_list[c]\n",
    "            url = BASE_URL + sc\n",
    "            ###### WHERE THE MAGIC HAPPENS ######\n",
    "            df = start_scraping(url, sr, sc, size, before)\n",
    "            b4v = df.iloc[-1]['created_utc']\n",
    "            b4.append(b4v)\n",
    "            list_df[c].append(df)\n",
    "            if(debug):\n",
    "                log_to_file(f'Looping through Coms/Subm, and subreddits: {c}')\n",
    "            c += 1\n",
    "    \n",
    "    if(coms_only):\n",
    "        df_before_c = make_before_vals_df(sub_red_1, sub_red_2, submission_comment[0], b4[0], b4[1])   # FOR COMS ONLY\n",
    "        df_before_c.to_csv('./before_coms.csv', index=False)\n",
    "    else:\n",
    "        df_before_c = make_before_vals_df(sub_red_1, sub_red_2, submission_comment[0], b4[0], b4[1])   # FOR SUB AND COMS\n",
    "        df_before_c.to_csv('./before_coms.csv', index=False)\n",
    "        df_before_s = make_before_vals_df(sub_red_1, sub_red_2, submission_comment[1], b4[2], b4[3])\n",
    "        df_before_s.to_csv('./before_subm.csv', index=False)\n",
    "        \n",
    "    if(debug):\n",
    "        log_to_file(f'Length of list_df: {len(list_df)}')\n",
    "    return list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "peaceful-decision",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_to_file(log_msg, log_file_name='', print_to_terminal=True, apply_date=True, show_info_tag = 1):\n",
    "    \"\"\"\n",
    "    Simple way to write to log.\n",
    "    Put in log message, and by default writes to '%Y-%m-%d_LOG.txt'\n",
    "    can change filename if you like with param 'log_file_name'\n",
    "    Also prints 'log_msg' to terminal by default with \n",
    "    param 'print_to_terminal=True'\n",
    "    \n",
    "    ARGUMENTS\n",
    "    log_msg: Message to wriute to log (and terminal if 'print_to_terminal' is set to True)\n",
    "    log_file_name (default: ''): log file name to write to; default means log will be named similarly to: YYYYmmdd_Log.txt\n",
    "    print_to_terminal (default: True): Prints to terminal if set to True\n",
    "    apply_date (default: True): Write date to log (each line generally begins with date and time)\n",
    "    show_info_tag (default: 1): Show [INFO] tag (after date/time, before message.) other tags include [WARN] and [ERROR]\n",
    "    \n",
    "    RETURN\n",
    "    Returns 'log_msg'\n",
    "    \"\"\"\n",
    "    info_tag = [ '', '[INFO] ','[WARN] ','[ERROR] ' ]\n",
    "    \n",
    "    if(len(log_file_name) == 0):\n",
    "        log_file_name = datetime.datetime.now().strftime('%Y-%m-%d_LOG.txt')\n",
    "\n",
    "    if(not os.path.exists(log_file_name)):\n",
    "        log_file = open(log_file_name, \"x\") # create file\n",
    "    \n",
    "    log_file = open(log_file_name, 'a') # appends msg to log\n",
    "    if(apply_date):\n",
    "        log_file.write(get_date_time())\n",
    "    if(show_info_tag > 0):\n",
    "        log_file.write(info_tag[show_info_tag])\n",
    "    log_file.write(log_msg)\n",
    "    log_file.write('\\n')\n",
    "    log_file.close()\n",
    "    if (print_to_terminal):\n",
    "        print(log_msg)\n",
    "    return log_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "large-denver",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_special_line_to_log(line_type=0, log_file_name='', print_to_terminal=True, apply_date=False, show_info_tag = 0):\n",
    "    \"\"\"\n",
    "    Uses 'log_to_file' Function, and has the same general functionality\n",
    "    Special commands to write to Log (and terminal if you want, but not by default):\n",
    "        0 = newline in log\n",
    "        1 = Long single line\n",
    "        2 = Long double line\n",
    "        3 = Short single line\n",
    "        4 = Short double line\n",
    "        5 = Long Star Line\n",
    "        6 = Short Star Line\n",
    "    \n",
    "    ARGUMENTS\n",
    "    line_type (default: 0): selects line type as outlined above (default is 'newline in log')\n",
    "    log_file_name (default: ''): Log filename to write to; default means log will be named similarly to: YYYYmmdd_Log.txt\n",
    "    print_to_terminal (default: True): Prints to terminal if set to True\n",
    "    apply_date (default: False): Write date to log; default set to False to show clear division in logs\n",
    "    show_info_tag (default: 1): Show [INFO] tag (after date/time, before message.) other tags include [WARN] and [ERROR]\n",
    "    \n",
    "    RETURNS\n",
    "    Returns call to log_to_file Function so it writes the special line\n",
    "    Since this function calls the log_to_file Function, this function also\n",
    "        returns the 'log_msg', in this case the special line (e.g. log_msg[line_type])\n",
    "    \"\"\"\n",
    "    log_msg = [\n",
    "        '',\n",
    "        '--------------------------------------------------------------------------------------------------',\n",
    "        '==================================================================================================',\n",
    "        '--------------------------------------------------------------------',\n",
    "        '====================================================================',\n",
    "        '**************************************************************************************************',\n",
    "        '********************************************************************'\n",
    "    ]\n",
    "    \n",
    "    return log_to_file(log_msg[line_type], log_file_name, print_to_terminal, apply_date, show_info_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "passing-programmer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date_time():\n",
    "    \"\"\" \n",
    "    Returns the date as a formatted string for logs: '%Y-%m-%d, %H:%M:%S - ' \n",
    "    \n",
    "    ARGUMENTS: None\n",
    "    \n",
    "    RETURN\n",
    "    Returns a date-time string in format: '%Y-%m-%d, %H:%M:%S '\n",
    "    \"\"\"\n",
    "    return datetime.datetime.now().strftime('%Y-%m-%d, %H:%M:%S ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spoken-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_before_csv(filename):\n",
    "    \"\"\"\n",
    "    Read in CSV with Before Value and returns the 2 values in an array\n",
    "    Function returns array of 2 zeros if no file is found\n",
    "    \n",
    "    ARGUMENTS\n",
    "    filename:\n",
    "    \n",
    "    RETURN\n",
    "    Returns 2 'before' parameters for pushshift.io's API, read in from CSV file\n",
    "    If no CSV file is found, array [0, 0] is returned\n",
    "    \"\"\"\n",
    "    print(datetime.datetime.now().strftime('\\n\\nSTART:  %Y-%m-%d, %H:%M'))\n",
    "    try:\n",
    "        b4v = pd.read_csv(filename)\n",
    "        b4v_s = b4v['before_value'].str.replace('_', '').apply(int)\n",
    "        log_to_file('READ IN \"BEFORE\" VALS: ')\n",
    "        log_to_file(str(b4v_s), show_info_tag=0)\n",
    "    except:\n",
    "        log_to_file(' *** FILE NOT FOUND: {filename} ***')\n",
    "        log_to_file('      - SETTING \"BEFORE\" VALUES TO ZERO (0)')\n",
    "        return [0, 0]\n",
    "    return b4v_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caroline-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_scraping(url, sub_reddit, sub_com, size=100, before=0, num_retry=10):\n",
    "    \"\"\"\n",
    "    Calls Function 'set_params_and_request' and prints/logs status\n",
    "    If there is a 500 error, function can retry; Default is 10 times\n",
    "    requests data is converted to json and put into a Data Frame\n",
    "    \n",
    "    ARGUMENTS\n",
    "    url: URL to use with API interface request\n",
    "    sub_reddit: Which subreddit to request information from\n",
    "    sub_com: string value, either \"submission\" or \"comment\" to denote what type of data to pull\n",
    "    size (default: 100): Number of submissions or comments to pull (maximum the API pulls is 100)\n",
    "    before (default: 0): Is used with a Date/Time object, Denotes pulling posts from before this date/time\n",
    "    num_retry (default: 10): Max Retries in case of network difficulties\n",
    "    \n",
    "    RETURN\n",
    "    Returns Data Frame of requests data\n",
    "    \"\"\"\n",
    "    time.sleep(10)\n",
    "    req = set_params_and_request(url, sub_reddit, size, before)\n",
    "    status = req.status_code\n",
    "    \n",
    "    log_tag = np.where(status == 200, 1, 2)\n",
    "    log_to_file(f'{sub_reddit} STATUS ({sub_com}): {status}', show_info_tag=log_tag)\n",
    "    \n",
    "    count = 0\n",
    "    while((status >= 500) & (count < num_retry)):\n",
    "        log_to_file('CONNECTION TIMED OUT', show_info_tag=2)\n",
    "        time.sleep(10)\n",
    "        req = set_params_and_request(url, sub_reddit, size, before)\n",
    "        status = req.status_code\n",
    "        log_to_file(f'{sub_reddit} STATUS ({sub_com}): {status}')\n",
    "        if(count == num_retry -1):\n",
    "            log_to_file('COULD NOT RESOLVE CONNECTION!', show_info_tag=3)\n",
    "        count += 1\n",
    "    \n",
    "    data = req.json()['data']\n",
    "    df_req = pd.DataFrame(data)\n",
    "    #write_to_csv(df_req, sub_reddit, sub_com)   # Old way\n",
    "    \n",
    "    return df_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dynamic-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params_and_request(url, sr, size, before, get_metadata = 'true'):\n",
    "    \"\"\"\n",
    "    Function sets up 'params' for pushshift.io reddit API and\n",
    "    \n",
    "    ARGUMENTS\n",
    "    url: URL to use with the subreddit data-pull request\n",
    "    sr: Subreddit to pull data from\n",
    "    size: Number of comments or submission to pull (API has a limit of 100)\n",
    "    before: Is used with a Date/Time object, Denotes pulling posts from before this date/time\n",
    "    get_metadata: feature that can be added to give some information about this pull request\n",
    "    \n",
    "    RETURN\n",
    "    Returns requests call: 'requests.get(url, params)'\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "\n",
    "    if(before <=0):\n",
    "        params = {\n",
    "            'subreddit': sr,\n",
    "            'size': size,\n",
    "            'metadata': get_metadata\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'subreddit': sr,\n",
    "            'size': size,\n",
    "            'before': before,    # 'created_utc'\n",
    "            'metadata': get_metadata\n",
    "        }\n",
    "    return requests.get(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "relative-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_before_vals_df(sr1, sr2, sub_com, value_sr1, value_sr2, to_terminal=True):\n",
    "    \"\"\"\n",
    "    'Before' values put into DataFrames and exported to CSV.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    sr1: Subreddit (in position 1)\n",
    "    sr2: Subreddit (in position 2)\n",
    "    sub_com: string value, either \"submission\" or \"comment\" to denote what type of data to pull\n",
    "    value_sr1: 'before' from last submission or comment from Subreddit in position 1\n",
    "    value_sr2: 'before' from last submission or comment from Subreddit in position 2\n",
    "    to_terminal: Before vals are written to log; When this is True, it will be written to  Terminal as well.\n",
    "    \n",
    "    RETURN\n",
    "    Returns 'before' value\n",
    "    \"\"\"\n",
    "    before_list = [[sr1, sub_com, value_sr1],\n",
    "                   [sr2, sub_com, value_sr2]]\n",
    "    before = pd.DataFrame(before_list, columns=['subreddit', 'sub_com', 'before_value'])\n",
    "    before['before_value'] = '_' + before['before_value'].apply(str)\n",
    "    \n",
    "    log_to_file(f'BEFORE VAL, {sr1} {sub_com}: {value_sr1}', print_to_terminal=to_terminal)\n",
    "    log_to_file(f'BEFORE VAL, {sr2} {sub_com}: {value_sr2}', print_to_terminal=to_terminal)\n",
    "    print(datetime.datetime.now().strftime('FINISH: %Y-%m-%d, %H:%M\\n'))\n",
    "    return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "announced-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(df_output, sr, sub_com, fn = ''):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a csv file. Takes args for \n",
    "    'sr' (subreddit) and for 'sub_com' (whether 'submission' \n",
    "    or 'comment') and 'fn (filename)' - if: fn == '', filename \n",
    "    is given default, e.g. '2021-04-24_ama_comments.csv'. \n",
    "    Output is designated to '../data/' and cannot be altered. \n",
    "    \n",
    "    ARGUMENTS\n",
    "    df_output: DataFrame to output to CSV file\n",
    "    sr: Subreddit associated with DataFrame\n",
    "    sub_com: string value, either \"submission\" or \"comment\" to denote what type of data to pull\n",
    "    fn (default: ''): Filename (default is similar to: 'YYYY-mm-dd_HHMM_subreddit_submission-or-comment.csv')\n",
    "    \n",
    "    RETURN\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../data/'):\n",
    "        log_to_file(f'  - Create Data Directory: ../data/')\n",
    "        os.makedirs('../data/')\n",
    "    if(len(fn) == 0):\n",
    "        fn = datetime.datetime.now().strftime('%Y-%m-%d_%H%M_') + sr + '_' + sub_com + '.csv'\n",
    "    output_file = '../data/' + fn\n",
    "    log_to_file('  - Writing to file: {fn}')\n",
    "    df_output.to_csv(output_file, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vietnamese-novel",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loop_call_scraper(num_loops, size=100, sub_reddit_1 = 'AMA', sub_reddit_2 = 'AskReddit', comments_only = False):\n",
    "    \"\"\"\n",
    "    Wrapper to the Function 'call_scraper' where it loops that function.\n",
    "    But it also grabs the returned list of DataFrames and compiles them \n",
    "    into four (or two) separate files per each subreddit and each \n",
    "    type (comments and if you want it, submissions - with \n",
    "    submissions is the default)\n",
    "    \n",
    "    ARGUMENTS\n",
    "    num_loops: Number of loops to execute; keeping default 'size'=100, \n",
    "        this number will pull this many comments/submissions, times 100, each\n",
    "    size (default: 100): Number of comments or submission to pull (API has a limit of 100)\n",
    "    sub_reddit_1 (default: 'AMA'): Subreddit in position 1\n",
    "    sub_reddit_2 (default: 'AskReddit'): Subreddit in position 2\n",
    "    comments_only (default: False): Only pulls Comments if set to True; by default will also pull Submissions.\n",
    "    \n",
    "    RETURN\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    ersion = '1.2.0.0'\n",
    "    \n",
    "    list_df_sr1_comments = []\n",
    "    list_df_sr2_comments = []\n",
    "    list_df_sr1_submissions = []\n",
    "    list_df_sr2_submissions = []\n",
    "    \n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(2)\n",
    "    log_to_file(f'  BEGIN LOOP SCRAPER - v{ersion}')\n",
    "    print_special_line_to_log(1)\n",
    "    \n",
    "    for i in range(num_loops):\n",
    "        log_to_file(f'(lOOP # {i + 1} of {num_loops})')\n",
    "        ldf = call_scraper(size, sub_reddit_1, sub_reddit_2, coms_only=comments_only)\n",
    "        if(comments_only):\n",
    "            list_df_sr1_comments.append(ldf[0][0])\n",
    "            list_df_sr2_comments.append(ldf[1][0])\n",
    "        else:\n",
    "            list_df_sr1_comments.append(ldf[0][0])\n",
    "            list_df_sr2_comments.append(ldf[1][0])\n",
    "            list_df_sr1_submissions.append(ldf[2][0])\n",
    "            list_df_sr2_submissions.append(ldf[3][0])\n",
    "            \n",
    "    log_to_file(f' *** Writing to CSV file(s) ***')\n",
    "    if(comments_only):\n",
    "        big_ol_sr1_comm_df = pd.concat(list_df_sr1_comments)\n",
    "        big_ol_sr2_comm_df = pd.concat(list_df_sr2_comments)\n",
    "        write_to_csv(big_ol_sr1_comm_df, sub_reddit_1, 'comments')\n",
    "        write_to_csv(big_ol_sr2_comm_df, sub_reddit_2, 'comments')\n",
    "    else:\n",
    "        big_ol_sr1_comm_df = pd.concat(list_df_sr1_comments)\n",
    "        big_ol_sr2_comm_df = pd.concat(list_df_sr2_comments)\n",
    "        big_ol_sr1_subm_df = pd.concat(list_df_sr1_submissions)\n",
    "        big_ol_sr2_subm_df = pd.concat(list_df_sr2_submissions)\n",
    "        write_to_csv(big_ol_sr1_comm_df, sub_reddit_1, 'comments')\n",
    "        write_to_csv(big_ol_sr2_comm_df, sub_reddit_2, 'comments')\n",
    "        write_to_csv(big_ol_sr1_subm_df, sub_reddit_1, 'submissions')\n",
    "        write_to_csv(big_ol_sr2_subm_df, sub_reddit_2, 'submissions')\n",
    "    \n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(1)\n",
    "    log_to_file(f'  FINISH LOOP SCRAPER')\n",
    "    print_special_line_to_log(2)\n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(0)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nuclear-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################\n",
    "###### CALL THE MAIN FUNCTION!!! ######\n",
    "#######################################\n",
    "\n",
    "\n",
    "#loop_call_scraper(10, size=200, sub_reddit_1 = 'AMA', sub_reddit_2 = 'AskReddit', comments_only = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fundamental-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================================================================\n",
      "  BEGIN LOOP SCRAPER - v1.1.0.1\n",
      "--------------------------------------------------------------------------------------------------\n",
      "(lOOP # 1 of 2)\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "  Begin Iteration of call_scraper\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "\n",
      "START:  2021-05-02, 20:47\n",
      "READ IN \"BEFORE\" VALS: \n",
      "0    1619521176\n",
      "1    1619548820\n",
      "Name: before_value, dtype: int64\n",
      "AMA STATUS (comment): 200\n",
      "AskReddit STATUS (comment): 200\n",
      "BEFORE VAL, AMA comment: 1619520758\n",
      "BEFORE VAL, AskReddit comment: 1619548819\n",
      "FINISH: 2021-05-02, 20:47\n",
      "\n",
      "(lOOP # 2 of 2)\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "  Begin Iteration of call_scraper\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "\n",
      "START:  2021-05-02, 20:47\n",
      "READ IN \"BEFORE\" VALS: \n",
      "0    1619520758\n",
      "1    1619548819\n",
      "Name: before_value, dtype: int64\n",
      "AMA STATUS (comment): 200\n",
      "AskReddit STATUS (comment): 200\n",
      "BEFORE VAL, AMA comment: 1619520025\n",
      "BEFORE VAL, AskReddit comment: 1619548816\n",
      "FINISH: 2021-05-02, 20:48\n",
      "\n",
      " *** Writing to CSV file(s) ***\n",
      "  - Writing to file: {fn}\n",
      "  - Writing to file: {fn}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "  FINISH LOOP SCRAPER\n",
      "==================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loop_call_scraper(20, size=100, sub_reddit_1 = 'AMA', sub_reddit_2 = 'AskReddit', comments_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-connection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
