# Notes, 2021-05-02


<h1 style="color:green;"> 2021-04-30_Models.ipynb </h1>
This group are models predicting `subreddit_binary` using `body` only, but only between Logisitc Regression and Naive Bayes(`MultinomialNB`). (Both used CountVectorizer and TfidfVectorizer, but generally each performed about the same, unless noted otherwise.)

<h2> Logistic Regression
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Checkmark_green.svg/1200px-Checkmark_green.svg.png" width="20" /> </h2>
Quite a bit of overfitting, where train data scores are around 80% - 90%, and test data is about 65% - 75%. Generally, ok-ish results - better than KNN and Naive Bayes.

<h2> Naive Bayes <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Pictogram_voting_oppose.svg/1200px-Pictogram_voting_oppose.svg.png" width="20" /> </h2>
Kind of all over the place with scores ranging from 50% to 70%. Train/Test data did better than introducing new data.

TfidfVectorize did slightly better than CountVecterize.
<h4 style="color:blue;"><em> NEED TO EXPERIMENT WITH LEMMA AND BAYES </em></h4>


<h2> KNN Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
CountVectorizer seems to work well with the KNN CLassifier Model. Test data has a F1 Score of 68% and a Recall of 78% - some overfitting since these scores are about 10% more than the relevant train data.



<h1 style="color:green;"> 2021-05-01_Models.ipynb </h1>
This group are models predicting `subreddit_binary` using `body` only. (Most used CountVectorizer and TfidfVectorizer, but generally each performed about the same, unless noted otherwise.)

<h2> Random Forest Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
A good amount of overfitting with all Train data in high 90s%. 

Test data performs at best around the mid-to-high 60s%

This might be good if I am able to add this to a voting-ensemble?


<h2> Decision Tree Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
Same as the Random Forest, but Test data performed in the low-to-mid 60s%.


<h2> Gradient Boosting Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Checkmark_green.svg/1200px-Checkmark_green.svg.png" width="20" /> </h2>
No overffitting! First model in this batch to exceed 70% in Test data across the board! (F1 score and Recall Score). Range is 60%-70%.


<h2> Bagging Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
Similar results to the Decision Tree Classifier


<h2> Ada Boost Clasifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Checkmark_green.svg/1200px-Checkmark_green.svg.png" width="20" /> </h2>
No overfitting, but still around 60%-70% - performs as good as Gradient Boosting Classifier, possibly better?

__Test data actually seems to perform better than some train data__.



<h1 style="color:green;"> 2021-05-01_Models_wo_body.ipynb </h1>
This group are models predicting `subreddit_binary` __*without*__ using `body` at all. All other columns have been converted to numerical values.the thought is possibly to combine use ofthese with probabilities of other results.


<h2> Regression Models <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
These did not perform well (as expected), but I tried it anyway. Specifically ElsaticNet and Linear Regression. I played around with Lasso/Ridge, but ElasticNet performed slightly better with using only a very small amount of Lasso. 

Both Models performed better when adding Polynomial Features. A small imporvement was made by adding OneHotEncoder, KNN Imputer, and Standard Scaler at the front of the pipeline (in that order).


<h2> Random Forest Classifier  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Pictogram_voting_oppose.svg/1200px-Pictogram_voting_oppose.svg.png" width="20" /> </h2>
Recall score of __95%__ on the test data!! Little overfitting, but lower accuracy. Possibly decent model, depending on what we are looking for.


<h2> Decision Tree Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
Little overfitting, decent precision, not so great Accuracy, terrible Recall.


<h2> Bagging Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
Quite a bit of overfitting; Scores are generally between 60% and 70% (training is between almost 80% and 90%)


<h2> Ada Boost Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
Little overfitting, scores range between 50% and 70%. Not the best model.


<h2> Gradient Boost Classifier <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/5f/Red_X.svg/768px-Red_X.svg.png" width="20" /> </h2>
Decent, not too much overfitting. Score ranges between 60% and 70%. Not bad


<h2> Logistic Regression <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Checkmark_green.svg/1200px-Checkmark_green.svg.png" width="20" /> </h2>
Excellent Recall - as good as Random Forest! Little overfitting. 

Will likely use this over Random Forest since Random Forest was inconsistent and did not do as well with the train/test data set. (If I am able to use this model at all)




<h1 style="color:green;"> 2021-05-02_voting-attempt.ipynb </h1>
Going to try using Voting to merge some redsults together


<h2> Logistic Regression, Ada Boost, Gradient Boost  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Pictogram_voting_oppose.svg/1200px-Pictogram_voting_oppose.svg.png" width="20" /> </h2>


<h2> can I merge my set of text-models with a non-text model??? </h2>
we can sure try!!

Trying out different settings, it was apparent that weighting Logistic Regression at 0.6, and the two Boosting Classifications at 0.2 each proved most effective for all cases.


<h2> GridSearch objects inside Voting Classifier </h2>
Took my 3 GridSearch objects (Ada Boost, Gradient Boost, and Logistic Regression), and put them through the Voting Classifier (weighted at 0.2, 0.2, 0.6 respectively).

<strong> Got an F1 Score of 72.76% (TP=920, FP=310, FN=379, TN=848) </strong>


<h2> Voting Classifier with Tfidf Vectorizer (w/o Gridsearch) </h2>
Took X_train and fit/transform with TfidfVectorizer, and tranformed X_test with it as well, before running it through the Voting Classifier with Ada Boost, Gradient Boost, and Logistic Regression, weighted at 0.2, 0.2, 0.6 respectively.

<strong> Got an F1 Score of 73.17% (TP=922, FP=308, FN=368, TN=859) </strong>



<h2> Voting Classifier with Count Vectorizer (w/o Gridsearch) </h2>
Took X_train and fit/transform with CountVectorizer, and tranformed X_test with it as well, before running it through the Voting Classifier with Ada Boost, Gradient Boost, and Logistic Regression, weighted at 0.2, 0.2, 0.6 respectively.

<strong> Got an F1 Score of 72.66% (TP=921, FP=309, FN=384, TN=843) </strong>

<h3> However... </h3>
When I put new data into the Model, results dropped about 10%. It looks like none of these are viable solutions at the moment.


<h2> Other ideas </h2>
I was hoping to weight my predictions using the other columns against these predictions, but it looks like the Voting Classifier works by using the same X-values (which makes sense). I do not think it would mattera anyway since my best results yielded about the same quality or worse.



<h1 style="color:green;"> Conclusion </h1>
It looks like good ol' Logistic Regression (wit hTfidf Vectorization) reigns supreme! It appears to be the best scoring and more consistent than any other varieties tested.... except lemmatized Naive Bayes, which I will be trying next!


