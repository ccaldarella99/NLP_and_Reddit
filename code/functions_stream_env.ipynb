{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "together-ukraine",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-remove",
   "metadata": {},
   "source": [
    "## Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prescribed-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Uncomment and) Run this cell so these libraries are available to other Jupyter Notebooks\n",
    "# !pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjacent-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cosmetic-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import f1_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metallic-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_binary_and_drop(df_in, drop='subreddit', repl_w_zero = 'AMA'):\n",
    "    \"\"\"\n",
    "    Add a Binarized version of a column, and then drop that column\n",
    "    \n",
    "    ARGUMENTS\n",
    "    df_in: The data frame to modify\n",
    "    drop: the column which will be read in, and then later dropped\n",
    "    repl_w_zero: The value to be replaced with a 0; everything else gets a 1\n",
    "    \n",
    "    RETURN\n",
    "    Returns the data frame it has modified\n",
    "    \"\"\"\n",
    "    bin_label = drop + '_binary'\n",
    "    df_in[bin_label] = np.where(df_in[drop] == repl_w_zero, 0, 1)\n",
    "    return df_in.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hybrid-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_binary(df_in, replace_column_name='subreddit', repl_w_zero = False):\n",
    "    \"\"\"\n",
    "    Change column to a Binarized version\n",
    "    \n",
    "    ARGUMENTS\n",
    "    df_in: The data frame to modify\n",
    "    replace_column_name: the column which will be Ninarized\n",
    "    repl_w_zero: The value to be replaced with a 0; everything else gets a 1\n",
    "    \n",
    "    RETURN\n",
    "    Returns the data frame it has modified\n",
    "    \"\"\"\n",
    "    if(replace_column_name in df_in):\n",
    "        df_in[replace_column_name] = np.where(df_in[replace_column_name] == repl_w_zero, 0, 1)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "honest-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_deleted_comments(df_in, col_to_modify='body', repl_w_nan = '[deleted]'):\n",
    "    \"\"\"\n",
    "    Find rows where body equals '[deleted]' and remove them from the set\n",
    "    This function also removes duplicate entries\n",
    "    \n",
    "    ARGUMENTS\n",
    "    df_in: The data frame to modify\n",
    "    col_to_modify: the column which will be read in and checked for 'repl_w_nan'\n",
    "    repl_w_nan: The value to be replaced with a NaN, and later be deleted (the whole row)\n",
    "    \n",
    "    RETURN\n",
    "    Returns the data frame it has modified\n",
    "    \"\"\"\n",
    "    df_in.drop_duplicates(inplace=True)\n",
    "    df_in[col_to_modify] = np.where(df_in[col_to_modify] == repl_w_nan, np.nan, df_in[col_to_modify])\n",
    "    return df_in.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mexican-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keywords(df_in, col_to_modify='body', remove_from = 'AMA'):\n",
    "    \"\"\"\n",
    "    Find keywords in body and remove them from there\n",
    "    \n",
    "    ARGUMENTS\n",
    "    df_in: The data frame to modify\n",
    "    col_to_modify: the column which will be read in and inspected\n",
    "    remove_from: The keyword(s) to be removed from 'col_to_modify'\n",
    "    \n",
    "    RETURN\n",
    "    Returns the data frame it has modified\n",
    "    \"\"\"\n",
    "    # df_in.drop_duplicates(inplace=True)\n",
    "    df_in[col_to_modify] = df_in[col_to_modify].str.replace(remove_from, '')\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecological-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols_cleaning(df_in, drop_cols=[]):\n",
    "    \"\"\"\n",
    "    Return DataFrame with typically unusable columns for building a model.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    df_in: The data frame to modify\n",
    "    drop_cols: columns to drop; if left empty, the following will be aut-assigned:\n",
    "        [ 'all_awardings', 'associated_award', 'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', \n",
    "       'author_flair_type', 'author_patreon_flair', 'awarders', 'collapsed_because_crowd_control', 'comment_type',\n",
    "       'created_utc', 'gildings', 'locked', 'permalink', 'retrieved_on', 'stickied', 'subreddit_id', \n",
    "       'top_awarded_type', 'treatment_tags', 'distinguished', 'author_cakeday']\n",
    "    \n",
    "    RETURN\n",
    "    Returns the data frame it has modified\n",
    "    \"\"\"\n",
    "    drop_cols = [ 'author', 'author_fullname', 'id', 'link_id', 'parent_id',\n",
    "        'all_awardings', 'associated_award', 'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', \n",
    "       'author_flair_type', 'author_patreon_flair', 'awarders', 'collapsed_because_crowd_control', 'comment_type',\n",
    "       'created_utc', 'gildings', 'locked', 'permalink', 'retrieved_on', 'stickied', 'subreddit_id', \n",
    "       'top_awarded_type', 'treatment_tags', 'distinguished', 'author_cakeday']\n",
    "    for c in drop_cols:\n",
    "        if(c in df_in):\n",
    "            df_in = df_in.drop(columns=c)\n",
    "\n",
    "    return df_in#.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "physical-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_post_length(df_in, delete_outliers=False, outliers=8000):\n",
    "    \"\"\"\n",
    "    Adds 'post_length' feature to DataFrame\n",
    "    \n",
    "    ARGUMENTS\n",
    "    df_in: DataFrame to be modified\n",
    "    delete_outliers (default: False): By default, does nothing. Set to True, will delete rows with 'body' characterlength greater than 'outliers'\n",
    "    outliers (default): when 'delete_outliers'=True, removes rows where 'body' character count is greater than this field; value of 4000 is recommended\n",
    "    \n",
    "    RETURN\n",
    "    Returns modified DataFrame\n",
    "    \"\"\"\n",
    "    df_in['post_length'] = df_in['body'].str.len()\n",
    "    if(delete_outliers):\n",
    "        df = df[df['post_length'] < outliers]\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demanding-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_search(pipe, params = {}):\n",
    "    \"\"\"\n",
    "    Simple pipeline in a GridSearch.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    pipe: insert a pipeline object (can use the make_pipeline function here, directly)\n",
    "    params: params for pipeline, default is an empty dict object\n",
    "    \n",
    "    RETURN\n",
    "    Returns GridSearchCV object\n",
    "    \"\"\"\n",
    "    gs = GridSearchCV(pipe, params, n_jobs=-1, verbose=2)\n",
    "    gs.fit(X_train, y_train)\n",
    "    train_score = gs.score(X_train, y_train)\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    best_parm = gs.best_params_\n",
    "    print(f'TRAIN: {train_score}')\n",
    "    print(f'TEST:  {test_score}')\n",
    "    print(f'BEST:  {best_parm}')\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "friendly-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_a_model(pipe, X_train, X_test = 0, y_train = 0, y_test = 0, params={}, is_classification=True, verbose=2):\n",
    "    \"\"\"\n",
    "    Early prototype to the Classification and Regression Model Classes\n",
    "    Probably should not use this, but left this here in case some old models were built using this\n",
    "    Takes in pipeline ad parameters as well as TTS X, and y; must denote whether a classification Model\n",
    "    Prints relevant scores\n",
    "    Returns GridSearchCV model\n",
    "    \"\"\"\n",
    "    if(type(y_train) == type(0)):\n",
    "        print(\" *** NEED A 'y' *** \")\n",
    "        return\n",
    "    \n",
    "    model = GridSearchCV(pipe, params, n_jobs=-1, verbose=verbose)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    if((type(X_test) == type(0)) | (type(y_test) == type(0))):\n",
    "        # treat as X and y only\n",
    "        if(is_classification):\n",
    "            accuracy = model.score(X_train)\n",
    "            f1_sc = f1_score(y_train, model.predict(X_train))\n",
    "            cm = confusion_matrix(y_train, model.predict(X_train))\n",
    "            scores = { 'Accuracy:': accuracy, 'F1 Score:': f1_sc, 'Confusion Matrix': cm }\n",
    "        else:\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            r2_train = model.score(X_train, y_train)\n",
    "            rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "            mse = mean_squared_error(y_train, y_train_pred)\n",
    "            mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            scores = { 'R2 Score': r2_train, 'RMSE': rmse, 'MSE': mse, 'MAE': mae }\n",
    "    else:\n",
    "        # score normally\n",
    "        if(is_classification):\n",
    "            y_pred = model.predict(X_test)\n",
    "            acc_train = model.score(X_train, y_train)\n",
    "            acc_test = model.score(X_test, y_test)\n",
    "            f1_train = f1_score(y_train, model.predict(X_train))\n",
    "            f1_test = f1_score(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            scores = { 'Accuracy, Train:': acc_train, 'Accuracy, Test:': acc_test, \n",
    "                       'F1 Train:': f1_train, 'F1 Test:': f1_test, 'Confusion Matrix': cm }\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2_train = model.score(X_train, y_train)\n",
    "            r2_test = model.score(X_test, y_test)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            scores = { 'R2 Train': r2_train, 'R2 Test': r2_test, 'RMSE': rmse, \n",
    "                       'MSE': mse, 'MAE': mae }\n",
    "            \n",
    "    for s,v in scores.items():\n",
    "        print(f'{s}: {v}')\n",
    "    \n",
    "    return model   # { 'Model': model, 'Scores': scores }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apart-blogger",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationModelX():\n",
    "    \"\"\"\n",
    "    Model (of your choice) for Classification\n",
    "    This Class takes one X and one y, and builds a Model for outside (untested) data\n",
    "    \n",
    "    ARGUMENTS (init)\n",
    "    pipe: input a pipeline for your GridSearchCV Model to use\n",
    "    X_train: your X (should be your full set of X) that trains your model\n",
    "    y_train: your y (should be your full set of y) that trains your model\n",
    "    params: Parameters for the pipline\n",
    "    verbose: Sets verbosity of Gridsearch (and values greater than 1 prints scores)\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    y_pred = None\n",
    "    y_pred_proba = None\n",
    "    accuracy_score = None\n",
    "    X_f1_score = None\n",
    "    confusion_matrix = None\n",
    "    recall_score = None\n",
    "    balanced_accuracy_score = None\n",
    "    precision_score = None\n",
    "    average_precision_score = None\n",
    "    predicted_score = None\n",
    "    roc_auc_score = None\n",
    "        \n",
    "    def __init__(self, pipe, X_train, y_train, params={}, verbose=2):\n",
    "        self.pipe = pipe\n",
    "        self.params = params\n",
    "        self.model = GridSearchCV(pipe, params, n_jobs=-1, verbose=verbose)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.best_params_ = self.model.best_params_\n",
    "        \n",
    "        self.set_y_pred(X_train)\n",
    "        self.set_y_pred_proba(X_train)\n",
    "        self.set_accuracy_score(y_train)\n",
    "        self.set_f1_score(y_train)\n",
    "        self.set_confusion_matrix(y_train)\n",
    "        self.set_recall_score(y_train)\n",
    "        self.set_balanced_accuracy_score(y_train)\n",
    "        self.set_precision_score(y_train)\n",
    "        self.set_average_precision_score(y_train)\n",
    "        self.set_roc_auc_score(y_train)\n",
    "        self.set_all_score()\n",
    "\n",
    "        if(verbose > 1):\n",
    "            print(f'\\nBaseline:')\n",
    "            print(f'  - count:\\n{y_train.value_counts()}')\n",
    "            print(f'  - percent:\\n{y_train.value_counts(normalize=True)}\\n')\n",
    "            self.print_stats()\n",
    "    \n",
    "    def score(X_new, y_new):\n",
    "        self.predicted_score = self.model.score(X_new, y_new)\n",
    "        print(self.predicted_score)\n",
    "        return self.predicted_score\n",
    "    \n",
    "    def print_stats(self):\n",
    "        print(f'PIPE: {self.pipe}\\n')\n",
    "        for s,v in self.scores.items():\n",
    "            print(f'  - {s}:\\n        {v}')\n",
    "        print(f'\\nbest params: {self.best_params_}')\n",
    "        \n",
    "    def plot_something(self):\n",
    "        print('this still needs to be built... ')\n",
    "        \n",
    "    def set_y_pred(self, X):\n",
    "        self.y_pred = self.model.predict(X)\n",
    "    \n",
    "    def set_y_pred_proba(self, X):\n",
    "        self.y_pred_proba = self.model.predict_proba(X)\n",
    "    \n",
    "    def set_accuracy_score(self, y_true):\n",
    "        self.accuracy_score = accuracy_score(y_true, self.y_pred)\n",
    "    \n",
    "    def set_f1_score(self, y_true):\n",
    "        self.X_f1_score = f1_score(y_true, self.y_pred)\n",
    "        \n",
    "    def set_confusion_matrix(self, y_true):\n",
    "        self.confusion_matrix = confusion_matrix(y_true, self.y_pred)\n",
    "    \n",
    "    def set_recall_score(self, y_true):\n",
    "        self.recall_score = recall_score(y_true, self.y_pred)\n",
    "    \n",
    "    def set_balanced_accuracy_score(self, y_true):\n",
    "        self.balanced_accuracy_score = balanced_accuracy_score(y_true, self.y_pred)\n",
    "        \n",
    "    def set_precision_score(self, y_true):\n",
    "        self.precision_score = precision_score(y_true, self.y_pred)\n",
    "    \n",
    "    def set_average_precision_score(self, y_true):\n",
    "        self.average_precision_score = average_precision_score(y_true, self.y_pred_proba[:, 1])\n",
    "        \n",
    "    def set_roc_auc_score(self, y_true):\n",
    "        self.roc_auc_score = roc_auc_score(y_true, self.y_pred_proba[:, 1])\n",
    "        \n",
    "    def set_all_score(self):\n",
    "        self.scores = {\n",
    "            'F1 Score': self.X_f1_score, \n",
    "            'Recall Score': self.recall_score, \n",
    "            'Accuracy': self.accuracy_score, \n",
    "            'Balanced Accuracy': self.balanced_accuracy_score, \n",
    "            'Precision Score': self.precision_score, \n",
    "            'Average Precision Score': self.average_precision_score, \n",
    "            'ROC AUC Score': self.roc_auc_score, \n",
    "            'Confusion Matrix': self.confusion_matrix\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "other-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel():\n",
    "    \"\"\"\n",
    "    Model (of your choice) for Classification\n",
    "    This Class takes X_train, X_test, y_train, and y_test and builds a Model\n",
    "    \n",
    "    ARGUMENTS (init)\n",
    "    pipe: input a pipeline for your GridSearchCV Model to use\n",
    "    X_train: your X that trains your model\n",
    "    y_train: your y that trains your model\n",
    "    X_train: your X to test your model with\n",
    "    y_train: your y to test your model with\n",
    "    params: Parameters for the pipline\n",
    "    mod_name: Model Name shown in DataFrame\n",
    "    verbose: Sets verbosity of GridSearchCV (1, 2, or 3)\n",
    "    print_results: set to True prints scores after __init__ runs, else prints DataFrame\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    train_scores = {}\n",
    "    y_pred = None\n",
    "    y_pred_proba = None\n",
    "    y_train_pred = None\n",
    "    y_train_pred_proba = None\n",
    "    train_accuracy_score = None\n",
    "    test_accuracy_score = None\n",
    "    train_f1_score = None\n",
    "    test_f1_score = None\n",
    "    train_confusion_matrix = None\n",
    "    test_confusion_matrix = None\n",
    "    train_true_positive = None\n",
    "    train_false_negative = None\n",
    "    train_false_positive = None\n",
    "    train_true_negative = None\n",
    "    test_true_positive = None\n",
    "    test_false_negative = None\n",
    "    test_false_positive = None\n",
    "    test_true_negative = None\n",
    "    train_recall_score = None\n",
    "    test_recall_score = None\n",
    "    train_balanced_accuracy_score = None\n",
    "    test_balanced_accuracy_score = None\n",
    "    train_precision_score = None\n",
    "    test_precision_score = None\n",
    "    train_average_precision_score = None\n",
    "    test_average_precision_score = None\n",
    "\n",
    "    train_calc_accuracy_score = None\n",
    "    test_calc_accuracy_score = None\n",
    "    train_calc_sensitivty_score = None\n",
    "    test_calc_sensitivty_score = None\n",
    "    train_calc_specficity_score = None\n",
    "    test_calc_specficity_score = None\n",
    "    train_calc_precision_score = None\n",
    "    test_calc_precision_score = None\n",
    "    \n",
    "    train_roc_auc_score = None\n",
    "    test_roc_auc_score = None\n",
    "    df = None\n",
    "        \n",
    "    def __init__(self, pipe, X_train, X_test, y_train, y_test, params={}, mod_name='', verbose=2, print_results=False):\n",
    "#         if(verbose > 1):\n",
    "        if(print_results):\n",
    "            print(datetime.datetime.now().strftime('START:  %Y-%m-%d, %H:%M:%S\\n'))\n",
    "        \n",
    "        self.model_name = mod_name\n",
    "        \n",
    "        self.pipe = pipe\n",
    "        self.params = params\n",
    "        self.model = GridSearchCV(pipe, params, n_jobs=-1, verbose=verbose)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.best_params_ = self.model.best_params_\n",
    "        if(len(mod_name) == 0):\n",
    "            self.model_name = str(type(self.pipe[-1])).split('.')[-1][:-2]\n",
    "        \n",
    "        self.y_pred = self.set_y_pred(X_test)\n",
    "        self.y_pred_proba = self.set_y_pred_proba(X_test)\n",
    "        self.y_train_pred = self.set_y_pred(X_train)\n",
    "        self.y_train_pred_proba = self.set_y_pred_proba(X_train)\n",
    "\n",
    "        self.train_accuracy_score = self.set_accuracy_score(y_train, self.y_train_pred)\n",
    "        self.test_accuracy_score = self.set_accuracy_score(y_test, self.y_pred)\n",
    "        self.train_f1_score = self.set_f1_score(y_train, self.y_train_pred)\n",
    "        self.test_f1_score = self.set_f1_score(y_test, self.y_pred)\n",
    "        \n",
    "        self.train_confusion_matrix = self.set_confusion_matrix(y_train, self.y_train_pred)\n",
    "        self.test_confusion_matrix = self.set_confusion_matrix(y_test, self.y_pred)\n",
    "        self.set_train_cm_features(self.train_confusion_matrix)\n",
    "        self.set_test_cm_features(self.test_confusion_matrix)\n",
    "        \n",
    "        self.train_recall_score = self.set_recall_score(y_train, self.y_train_pred)\n",
    "        self.test_recall_score = self.set_recall_score(y_test, self.y_pred)\n",
    "        self.train_balanced_accuracy_score = self.set_balanced_accuracy_score(y_train, self.y_train_pred)\n",
    "        self.test_balanced_accuracy_score = self.set_balanced_accuracy_score(y_test, self.y_pred)\n",
    "        self.train_precision_score = self.set_precision_score(y_train, self.y_train_pred)\n",
    "        self.test_precision_score = self.set_precision_score(y_test, self.y_pred)\n",
    "        self.train_average_precision_score = self.set_average_precision_score(y_train, self.y_train_pred_proba)\n",
    "        self.test_average_precision_score = self.set_average_precision_score(y_test, self.y_pred_proba)\n",
    "        self.train_roc_auc_score = self.set_roc_auc_score(y_train, self.y_train_pred_proba)\n",
    "        self.test_roc_auc_score = self.set_roc_auc_score(y_test, self.y_pred_proba)\n",
    "        \n",
    "        self.train_calc_accuracy_score = self.set_calc_accuracy_score(self.train_true_positive, \n",
    "                                                                      self.train_false_negative, \n",
    "                                                                      self.train_false_positive, \n",
    "                                                                      self.train_true_negative)\n",
    "        self.test_calc_accuracy_score = self.set_calc_accuracy_score(self.test_true_positive, \n",
    "                                                                     self.test_false_negative, \n",
    "                                                                     self.test_false_positive, \n",
    "                                                                     self.test_true_negative)\n",
    "        self.train_calc_sensitivty_score = self.set_calc_sensitivty_score(self.train_true_positive, \n",
    "                                                                          self.train_false_negative)\n",
    "        self.test_calc_sensitivty_score = self.set_calc_sensitivty_score(self.test_true_positive,\n",
    "                                                                         self.test_false_negative)\n",
    "        self.train_calc_specficity_score = self.set_calc_specficity_score(self.train_true_negative, \n",
    "                                                                          self.train_false_positive)\n",
    "        self.test_calc_specficity_score = self.set_calc_specficity_score(self.test_true_negative, \n",
    "                                                                         self.test_false_positive)\n",
    "        self.train_calc_precision_score = self.set_calc_precision_score(self.train_true_positive,\n",
    "                                                                        self.train_false_positive)\n",
    "        self.test_calc_precision_score = self.set_calc_precision_score(self.test_true_positive,\n",
    "                                                                       self.test_false_positive)\n",
    "        \n",
    "        self.set_all_score()\n",
    "        self.set_DataFrame()\n",
    "\n",
    "        if(print_results):# & verbose > 1):\n",
    "            # print(datetime.datetime.now().strftime('START:  %Y-%m-%d, %H:%M:%S'))\n",
    "            print(f'\\nBaseline (y_train):')\n",
    "            print(f'  - count:\\n{y_train.value_counts()}')\n",
    "            print(f'  - percent:\\n{y_train.value_counts(normalize=True)}\\n')\n",
    "            print(f'\\nBaseline (y_test):')\n",
    "            print(f'  - count:\\n{y_test.value_counts()}')\n",
    "            print(f'  - percent:\\n{y_test.value_counts(normalize=True)}\\n')\n",
    "            print(f'\\nSTATS (y_train):')\n",
    "            self.print_train_stats()\n",
    "            print(f'\\nSTATS (y_test):')\n",
    "            self.print_stats()\n",
    "            print(f'  - Model, BEST SCORE: {self.model.best_score_}')\n",
    "            print(datetime.datetime.now().strftime('\\nFINISH: %Y-%m-%d, %H:%M:%S'))\n",
    "#         elif(verbose > 1):\n",
    "#             self.DataFrame()\n",
    "    \n",
    "    def DataFrame(self):\n",
    "        return self.df\n",
    "    \n",
    "    def set_DataFrame(self):\n",
    "        train_df = pd.DataFrame(np.array([v for k,v in self.train_scores.items()]), \n",
    "                               index=[k for k,v in self.train_scores.items()], \n",
    "                               columns=[self.model_name])\n",
    "        test_df = pd.DataFrame(np.array([v for k,v in self.scores.items()]), \n",
    "                               index=[k for k,v in self.scores.items()], \n",
    "                               columns=[self.model_name])\n",
    "        self.df = pd.concat([test_df, train_df])\n",
    "    \n",
    "    def set_all_score(self):\n",
    "        self.scores = {\n",
    "            'F1 Score': self.test_f1_score, \n",
    "            'Recall Score': self.test_recall_score, \n",
    "            'Accuracy': self.test_accuracy_score, \n",
    "            'Balanced Accuracy': self.test_balanced_accuracy_score, \n",
    "            'Precision Score': self.test_precision_score, \n",
    "            'Average Precision Score': self.test_average_precision_score, \n",
    "            'ROC AUC Score': self.test_roc_auc_score, \n",
    "            #'Confusion Matrix': self.test_confusion_matrix\n",
    "            'True Positive': self.test_true_positive,\n",
    "            'False Negative': self.test_false_negative,\n",
    "            'False Positive': self.test_false_positive,\n",
    "            'True Negative': self.test_true_negative,\n",
    "            'Calculated Accuracy': self.test_calc_accuracy_score,\n",
    "            'Calculated Precision': self.test_calc_precision_score,\n",
    "            'Calculated Sensitivity': self.test_calc_sensitivty_score,\n",
    "            'Calculated Specificity': self.test_calc_specficity_score\n",
    "        }\n",
    "        self.train_scores = {\n",
    "            'Train F1 Score': self.train_f1_score, \n",
    "            'Train Recall Score': self.train_recall_score, \n",
    "            'Train Accuracy': self.train_accuracy_score, \n",
    "            'Train Balanced Accuracy': self.train_balanced_accuracy_score, \n",
    "            'Train Precision Score': self.train_precision_score, \n",
    "            'Train Average Precision Score': self.train_average_precision_score, \n",
    "            'Train ROC AUC Score': self.train_roc_auc_score, \n",
    "            #'Train Confusion Matrix': self.train_confusion_matrix\n",
    "            'Train True Positive': self.train_true_positive,\n",
    "            'Train False Negative': self.train_false_negative,\n",
    "            'Train False Positive': self.train_false_positive,\n",
    "            'Train True Negative': self.train_true_negative,\n",
    "            'Train Calculated Accuracy': self.train_calc_accuracy_score,\n",
    "            'Train Calculated Precision': self.train_calc_precision_score,\n",
    "            'Train Calculated Sensitivity': self.train_calc_sensitivty_score,\n",
    "            'Train Calculated Specificity': self.train_calc_specficity_score\n",
    "        }\n",
    "    \n",
    "    def print_stats(self):\n",
    "        for s,v in self.scores.items():\n",
    "            print(f'  - {s}:\\n        {v}')\n",
    "        print(f'\\nPIPE: {self.pipe}')\n",
    "        print(f'\\nbest params: {self.best_params_}')\n",
    "        \n",
    "    def print_train_stats(self):\n",
    "        for s,v in self.train_scores.items():\n",
    "            print(f'  - {s}:\\n        {v}')\n",
    "        \n",
    "    def set_y_pred(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def set_y_pred_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def set_accuracy_score(self, y_true, y_predict):\n",
    "        return accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    def set_f1_score(self, y_true, y_predict):\n",
    "        return f1_score(y_true, y_predict)\n",
    "        \n",
    "    def set_confusion_matrix(self, y_true, y_predict):\n",
    "        return confusion_matrix(y_true, y_predict)\n",
    "    \n",
    "    def set_train_cm_features(self, confusion_matrix):\n",
    "        tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "        self.train_true_positive = tn  # confusion_matrix[0][0]\n",
    "        self.train_false_negative = fp # confusion_matrix[0][1]\n",
    "        self.train_false_positive = fn # confusion_matrix[1][0]\n",
    "        self.train_true_negative = tp  # confusion_matrix[1][1]\n",
    "    \n",
    "    def set_test_cm_features(self, confusion_matrix):\n",
    "        tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "        self.test_true_positive = tn  # confusion_matrix[0][0]\n",
    "        self.test_false_negative = fp # confusion_matrix[0][1]\n",
    "        self.test_false_positive = fn # confusion_matrix[1][0]\n",
    "        self.test_true_negative = tp  # confusion_matrix[1][1]\n",
    "    \n",
    "    def set_recall_score(self, y_true, y_predict):\n",
    "        return recall_score(y_true, y_predict)\n",
    "    \n",
    "    def set_balanced_accuracy_score(self, y_true, y_predict):\n",
    "        return balanced_accuracy_score(y_true, y_predict)\n",
    "        \n",
    "    def set_precision_score(self, y_true, y_predict):\n",
    "        return precision_score(y_true, y_predict)\n",
    "    \n",
    "    def set_average_precision_score(self, y_true, y_predict_proba):\n",
    "        return average_precision_score(y_true, y_predict_proba[:, 1])\n",
    "        \n",
    "    def set_roc_auc_score(self, y_true, y_predict_proba):\n",
    "        return roc_auc_score(y_true, y_predict_proba[:, 1])\n",
    "    \n",
    "    def set_calc_accuracy_score(self, tp, fp, fn, tn):\n",
    "        return (tp+tn)/(tp + fp + tn + fn)\n",
    "    \n",
    "    def set_calc_sensitivty_score(self, tp, fn):\n",
    "        return tp/(tp+fn)\n",
    "    \n",
    "    def set_calc_specficity_score(self, tn, fp):\n",
    "        return tn/(tn+fp)\n",
    "    \n",
    "    def set_calc_precision_score(self, tp, fp):\n",
    "        return tp/(tp+fp)\n",
    "    \n",
    "    def plot_confusion_matrix(self, X_test, y_test):\n",
    "        plot_confusion_matrix(self.model, X_test, y_test);\n",
    "    \n",
    "    def check_y(self, y_true, y_pred):\n",
    "        y_compare = np.where(y_true == y_pred, 1, 0)\n",
    "        print(np.asarray((np.unique(y_compare, return_counts=True))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spoken-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel():\n",
    "    \"\"\"\n",
    "    Model (of your choice) for Classification\n",
    "    This Class takes X_train, X_test, y_train, and y_test and builds a Model\n",
    "    \n",
    "    ARGUMENTS (init)\n",
    "    pipe: input a pipeline for your GridSearchCV Model to use\n",
    "    X_train: your X that trains your model\n",
    "    y_train: your y that trains your model\n",
    "    X_train: your X to test your model with\n",
    "    y_train: your y to test your model with\n",
    "    params: Parameters for the pipline\n",
    "    mod_name: Model Name shown in DataFrame\n",
    "    verbose: Sets verbosity of GridSearchCV (1, 2, or 3)\n",
    "    print_results: set to True prints scores after __init__ runs, else prints DataFrame\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    train_scores = {}\n",
    "    y_pred = None\n",
    "    y_train_pred = None\n",
    "    train_r2_score = None\n",
    "    test_r2_score = None\n",
    "    train_rmse = None\n",
    "    test_rmse = None\n",
    "    train_mse = None\n",
    "    test_mse = None\n",
    "    train_mae = None\n",
    "    test_mae = None\n",
    "    df = None\n",
    "    y_thresh = None\n",
    "        \n",
    "    def __init__(self, pipe, X_train, X_test, y_train, y_test, params={}, mod_name='', \n",
    "                 round_y_threshold = 0.5, invert_y=False, verbose=2, print_results=False):\n",
    "#         if(verbose > 1):\n",
    "        if(print_results):\n",
    "            print(datetime.datetime.now().strftime('START:  %Y-%m-%d, %H:%M:%S\\n'))\n",
    "        \n",
    "        self.model_name = mod_name\n",
    "        self.y_thresh = round_y_threshold\n",
    "\n",
    "        self.pipe = pipe\n",
    "        self.params = params\n",
    "        self.model = GridSearchCV(pipe, params, n_jobs=-1, verbose=verbose)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        self.best_params_ = self.model.best_params_\n",
    "        if(len(mod_name) == 0):\n",
    "            self.model_name = str(type(self.pipe[-1])).split('.')[-1][:-2]\n",
    "        \n",
    "        if(invert_y):\n",
    "            self.y_pred = self.set_y_pred_inv(X_test)\n",
    "            self.y_train_pred = self.set_y_pred_inv(X_train)\n",
    "        else:\n",
    "            self.y_pred = self.set_y_pred(X_test)\n",
    "            self.y_train_pred = self.set_y_pred(X_train)\n",
    "\n",
    "        self.train_r2_score = self.set_r2_score(y_train, self.y_train_pred)\n",
    "        self.test_r2_score = self.set_r2_score(y_test, self.y_pred)\n",
    "        self.train_rmse = self.set_rmse(y_train, self.y_train_pred)\n",
    "        self.test_rmse = self.set_rmse(y_test, self.y_pred)\n",
    "        self.train_mse = self.set_mse(y_train, self.y_train_pred)\n",
    "        self.test_mse = self.set_mse(y_test, self.y_pred)\n",
    "        self.train_mae = self.set_mae(y_train, self.y_train_pred)\n",
    "        self.test_mae = self.set_mae(y_test, self.y_pred)\n",
    "        \n",
    "        self.set_all_score()\n",
    "        self.set_DataFrame()\n",
    "\n",
    "        if(print_results):# & verbose > 1):\n",
    "            self.print_results()\n",
    "    \n",
    "    def DataFrame(self):\n",
    "        return self.df\n",
    "    \n",
    "    def set_DataFrame(self):\n",
    "        train_df = pd.DataFrame(np.array([v for k,v in self.train_scores.items()]), \n",
    "                               index=[k for k,v in self.train_scores.items()], \n",
    "                               columns=[self.model_name])\n",
    "        test_df = pd.DataFrame(np.array([v for k,v in self.scores.items()]), \n",
    "                               index=[k for k,v in self.scores.items()], \n",
    "                               columns=[self.model_name])\n",
    "        self.df = pd.concat([test_df, train_df])#, axis=1)\n",
    "    \n",
    "    def print_results(self, y_train, y_test):\n",
    "        # print(datetime.datetime.now().strftime('START:  %Y-%m-%d, %H:%M:%S'))\n",
    "        print(f'\\nBaseline (y_train):')\n",
    "        print(f'  - count:\\n{y_train.value_counts()}')\n",
    "        print(f'  - percent:\\n{y_train.value_counts(normalize=True)}\\n')\n",
    "        print(f'\\nBaseline (y_test):')\n",
    "        print(f'  - count:\\n{y_test.value_counts()}')\n",
    "        print(f'  - percent:\\n{y_test.value_counts(normalize=True)}\\n')\n",
    "        print(f'\\nSTATS (y_train):')\n",
    "        self.print_train_stats()\n",
    "        print(f'\\nSTATS (y_test):')\n",
    "        self.print_stats()\n",
    "        print(f'  - Model, BEST SCORE: {self.model.best_score_}')\n",
    "        # print(datetime.datetime.now().strftime('\\nFINISH: %Y-%m-%d, %H:%M:%S'))\n",
    "\n",
    "    \n",
    "    def set_all_score(self):\n",
    "        self.scores = {\n",
    "            'R2 Score': self.test_r2_score, \n",
    "            'RMSE': self.test_rmse, \n",
    "            'MSE': self.test_mse, \n",
    "            'MAE': self.test_mae, \n",
    "        }\n",
    "        self.train_scores = {\n",
    "            'Train R2 Score': self.train_r2_score, \n",
    "            'Train RMSE': self.train_rmse, \n",
    "            'Train MSE': self.train_mse, \n",
    "            'Train MAE': self.train_mae, \n",
    "        }\n",
    "    \n",
    "    def print_stats(self):\n",
    "        for s,v in self.scores.items():\n",
    "            print(f'  - {s}:\\n        {v}')\n",
    "        print(f'\\nPIPE: {self.pipe}')\n",
    "        print(f'\\nbest params: {self.best_params_}')\n",
    "    \n",
    "    def print_train_stats(self):\n",
    "        for s,v in self.train_scores.items():\n",
    "            print(f'  - {s}:\\n        {v}')\n",
    "    \n",
    "    def set_y_pred(self, X):\n",
    "        if(self.y_thresh == -1):\n",
    "            return self.model.predict(X)\n",
    "        else:\n",
    "            return np.abs(np.where(self.model.predict(X) < self.y_thresh, 0, 1))\n",
    "    \n",
    "    def set_y_pred_inv(self, X):\n",
    "        return np.abs(np.where(self.model.predict(X) < self.y_thresh, 0, 1) - 1)\n",
    "    \n",
    "    def set_r2_score(self, y_true, y_predict):\n",
    "        return r2_score(y_true, y_predict)\n",
    "        \n",
    "    def set_rmse(self, y_true, y_predict):\n",
    "        return mean_squared_error(y_true, y_predict, squared=False)\n",
    "    \n",
    "    def set_mse(self, y_true, y_predict):\n",
    "        return mean_squared_error(y_true, y_predict, squared=True)\n",
    "        \n",
    "    def set_mae(self, y_true, y_predict):\n",
    "        return mean_absolute_error(y_true, y_predict)\n",
    "    \n",
    "    def check_y(self, y_true, y_pred):\n",
    "        y_compare = np.where(y_true == y_pred, 1, 0)\n",
    "        print(np.asarray((np.unique(y_compare, return_counts=True))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "becoming-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanUp:\n",
    "    \"\"\"\n",
    "    Class takes in DataFrame and cleans it upon initialization, and keeps the cleaned version as part of itself.\n",
    "    \"\"\"\n",
    "    df = None\n",
    "    drop_cols = [ 'author', 'author_fullname', 'id', 'link_id', 'parent_id',\n",
    "        'all_awardings', 'associated_award', 'author_flair_background_color', 'author_flair_css_class',\n",
    "       'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', \n",
    "       'author_flair_type', 'author_patreon_flair', 'awarders', 'collapsed_because_crowd_control', 'comment_type',\n",
    "       'created_utc', 'gildings', 'locked', 'permalink', 'retrieved_on', 'stickied', 'subreddit_id', \n",
    "       'top_awarded_type', 'treatment_tags', 'distinguished', 'author_cakeday']\n",
    "    binarize = [ 'author_premium', 'is_submitter', 'no_follow', 'send_replies' ]\n",
    "    \n",
    "    def __init__(self, df_in):\n",
    "        self.df = df_in.copy()\n",
    "\n",
    "        self.df = self.drop_cols_cleaning()\n",
    "        for b in self.binarize:\n",
    "            if(b in self.df.columns):\n",
    "                self.df = self.replace_with_binary(b)\n",
    "        if('body' in self.df.columns):\n",
    "            self.df = self.remove_deleted_comments()\n",
    "            self.df = self.remove_keywords(remove_from='AMA')\n",
    "            self.df = self.remove_keywords(remove_from='AskReddit')\n",
    "            self.df = self.add_post_length()\n",
    "        if('subreddit' in self.df.columns):\n",
    "            self.df = self.add_binary_and_drop()\n",
    "#         self.df = self.df.drop_duplicates()\n",
    "    \n",
    "    def replace_with_binary(self, replace_column_name, repl_w_zero = False):\n",
    "        \"\"\"\n",
    "        Change column to a Binarized version\n",
    "\n",
    "        ARGUMENTS\n",
    "        df_in: The data frame to modify\n",
    "        replace_column_name: the column which will be Ninarized\n",
    "        repl_w_zero: The value to be replaced with a 0; everything else gets a 1\n",
    "\n",
    "        RETURN\n",
    "        Returns the data frame it has modified\n",
    "        \"\"\"\n",
    "        self.df[replace_column_name] = np.where(self.df[replace_column_name] == repl_w_zero, 0, 1)\n",
    "        return self.df\n",
    "    \n",
    "    def remove_deleted_comments(self, col_to_modify='body', repl_w_nan = '[deleted]'):\n",
    "        \"\"\"\n",
    "        Find rows where body equals '[deleted]' and remove them from the set\n",
    "        This function also removes duplicate entries\n",
    "\n",
    "        ARGUMENTS\n",
    "        df_in: The data frame to modify\n",
    "        col_to_modify: the column which will be read in and checked for 'repl_w_nan'\n",
    "        repl_w_nan: The value to be replaced with a NaN, and later be deleted (the whole row)\n",
    "\n",
    "        RETURN\n",
    "        Returns the data frame it has modified\n",
    "        \"\"\"\n",
    "        # self.df.drop_duplicates(inplace=True)\n",
    "        self.df[col_to_modify] = np.where(self.df[col_to_modify] == repl_w_nan, np.nan, self.df[col_to_modify])\n",
    "        return self.df.dropna()\n",
    "    \n",
    "    def remove_keywords(self, remove_from = 'AMA', col_to_modify='body'):\n",
    "        \"\"\"\n",
    "        Find keywords in body and remove them from there\n",
    "\n",
    "        ARGUMENTS\n",
    "        df_in: The data frame to modify\n",
    "        col_to_modify: the column which will be read in and inspected\n",
    "        remove_from: The keyword(s) to be removed from 'col_to_modify'\n",
    "\n",
    "        RETURN\n",
    "        Returns the data frame it has modified\n",
    "        \"\"\"\n",
    "        self.df[col_to_modify] = self.df[col_to_modify].str.replace(remove_from, '')\n",
    "        return self.df\n",
    "    \n",
    "    def drop_cols_cleaning(self):   #, drop_cols):\n",
    "        \"\"\"\n",
    "        Return DataFrame with typically unusable columns for building a model.\n",
    "\n",
    "        ARGUMENTS\n",
    "        df_in: The data frame to modify\n",
    "        drop_cols: columns to drop; if left empty, the following will be aut-assigned:\n",
    "            [ 'all_awardings', 'associated_award', 'author_flair_background_color', 'author_flair_css_class',\n",
    "           'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', \n",
    "           'author_flair_type', 'author_patreon_flair', 'awarders', 'collapsed_because_crowd_control', 'comment_type',\n",
    "           'created_utc', 'gildings', 'locked', 'permalink', 'retrieved_on', 'stickied', 'subreddit_id', \n",
    "           'top_awarded_type', 'treatment_tags', 'distinguished', 'author_cakeday']\n",
    "\n",
    "        RETURN\n",
    "        Returns the data frame it has modified\n",
    "        \"\"\"\n",
    "        for c in self.drop_cols:\n",
    "            if(c in self.df):\n",
    "                self.df = self.df.drop(columns=c)\n",
    "\n",
    "        return self.df#.drop(columns=self.drop_cols)\n",
    "    \n",
    "    def add_post_length(self):\n",
    "        self.df['post_length'] = self.df['body'].str.len()\n",
    "        return self.df\n",
    "    \n",
    "    def add_binary_and_drop(self, drop_label='subreddit', repl_w_zero = 'AMA'):\n",
    "        \"\"\"\n",
    "        Add a Binarized version of a column, and then drop that column\n",
    "\n",
    "        ARGUMENTS\n",
    "        df_in: The data frame to modify\n",
    "        drop: the column which will be read in, and then later dropped\n",
    "        repl_w_zero: The value to be replaced with a 0; everything else gets a 1\n",
    "\n",
    "        RETURN\n",
    "        Returns the data frame it has modified\n",
    "        \"\"\"\n",
    "        bin_label = drop_label + '_binary'\n",
    "        self.df[bin_label] = np.where(self.df[drop_label] == repl_w_zero, 0, 1)\n",
    "        return self.df.drop(columns=drop_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loose-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_columns(df_in, alter_rows=False):\n",
    "    \"\"\"\n",
    "    Function that Cleans a DataFrame as outlined by the EDA/Cleaning process\n",
    "    \"\"\"\n",
    "    binarize = [ 'author_premium', 'is_submitter', 'no_follow', 'send_replies' ]\n",
    "    \n",
    "    df_in = drop_cols_cleaning(df_in)\n",
    "    for b in binarize:\n",
    "        df_in = replace_with_binary(b)\n",
    "    if('body' in df_in.columns):\n",
    "        df_in = remove_deleted_comments(df_in)\n",
    "        df_in = remove_keywords(df_in, remove_from='AMA')\n",
    "        df_in = remove_keywords(df_in, remove_from='AskReddit')\n",
    "        df_in = add_post_length(df_in, True, 4000)\n",
    "    if(alter_rows):\n",
    "        df_in = df_in.drop_duplicates()\n",
    "    if('subreddit' in df_in.columns):\n",
    "        df_in = add_binary_and_drop(df_in)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "forbidden-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_stats(y_true, y_predict, print_values=False):\n",
    "    \"\"\"\n",
    "    Quick function that returns Classification metrics.\n",
    "    \n",
    "    ARGUMENTS\n",
    "    y_true: true/actual value of y to use for metrics\n",
    "    y_predict: predicted value of y to use for metrics\n",
    "    print_values (default: False): prints values to terminal\n",
    "    \n",
    "    RETURN\n",
    "    Returns Dictionary of the Classification Metric Scores\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_predict).ravel()\n",
    "    accuracy_calc = (tp+tn)/(tp + fp + tn + fn)\n",
    "    sensitivty_calc = tp/(tp+fn)\n",
    "    specficity_calc = tn/(tn+fp)\n",
    "    precision_calc = tp/(tp+fp)\n",
    "    \n",
    "    scores = {\n",
    "        'F1 Score': f1_score(y_true, y_predict),\n",
    "        'Recall Score': recall_score(y_true, y_predict), \n",
    "        'Accuracy Score': accuracy_score(y_true, y_predict),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_predict),\n",
    "        'Precision Score': precision_score(y_true, y_predict), \n",
    "        'True Positive': tp,\n",
    "        'False Negative': fn,\n",
    "        'False Positive': fp,\n",
    "        'True Negative': tn,\n",
    "        'Calculated Accuracy': accuracy_calc,\n",
    "        'Calculated Precision': precision_calc,\n",
    "        'Calculated Sensitivity': sensitivty_calc,\n",
    "        'Calculated Specificity': specficity_calc\n",
    "    }\n",
    "    \n",
    "    if(print_values):\n",
    "        for s,v in scores.items():\n",
    "            print(f'  - {s}:\\n        {v}')\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-place",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-avenue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
