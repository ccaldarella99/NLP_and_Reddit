{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "russian-upgrade",
   "metadata": {},
   "source": [
    "# Webscraper (for reddit.com, project 3)\n",
    "(Example: [https://www.youtube.com/watch?v=AcrjEWsMi_E](https://www.youtube.com/watch?v=AcrjEWsMi_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "neutral-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last Updated: 2021-04-24, 00:14\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complete-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "BASE_URL = 'https://api.pushshift.io/reddit/search/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continuous-territory",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_scraper(size, sub_red_1 = 'AMA', sub_red_2 = 'AskReddit', coms_only=False, before=1, debug=False):\n",
    "    \"\"\"\n",
    "    Calls Reddit API and grabs two subreddit's comments and submissions \n",
    "    Takes 'size' and subreddit' params and passes them to the API\n",
    "    By default, this grabs comments and submissions, resulting in 4 Data Frames\n",
    "    coms_only boolean ONLY grabs comments (2 Data Frames, no submissions)\n",
    "    Function reads in files 'before_coms.csv' and 'before_subm.csv' to get\n",
    "    the oldest 'created_utc' from the last run (so it can pickup where it left off)\n",
    "    which is passed to the params as the value for 'before'\n",
    "    \n",
    "    \"\"\"\n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(3)\n",
    "    log_to_file(f'  Begin Iteration of call_scraper')\n",
    "    print_special_line_to_log(3)\n",
    "    \n",
    "    c = 0\n",
    "    sub_reddit = [sub_red_1, sub_red_2]\n",
    "    b4 = []\n",
    "    list_df = [[],[],[],[]]\n",
    "    \n",
    "    if(coms_only):\n",
    "        submission_comment = ['comment']    # FOR COMS ONLY\n",
    "    else:\n",
    "        submission_comment = ['comment', 'submission']   # FOR SUB AND COMS\n",
    "    \n",
    "    # read in before.csv values as a list, and apply them \n",
    "    if(coms_only):\n",
    "        old_before_list = read_in_before_csv('before_coms.csv')   # FOR COMS ONLY\n",
    "    else:\n",
    "        ob4_coms = read_in_before_csv('before_coms.csv')   # FOR SUB AND COMS\n",
    "        ob4_subm = read_in_before_csv('before_subm.csv')\n",
    "        old_before_list = [ ob4_coms[0], ob4_coms[1], ob4_subm[0], ob4_subm[1] ]\n",
    "    \n",
    "    for sc in submission_comment:\n",
    "        for sr in sub_reddit:\n",
    "            before = 0\n",
    "            if(c < len(old_before_list)):\n",
    "                before = old_before_list[c]\n",
    "            url = BASE_URL + sc\n",
    "            ###### WHERE THE MAGIC HAPPENS ######\n",
    "            df = start_scraping(url, sr, sc, size, before)\n",
    "            b4v = df.iloc[-1]['created_utc']\n",
    "            b4.append(b4v)\n",
    "            list_df[c].append(df)\n",
    "            if(debug):\n",
    "                log_to_file(f'Looping through Coms/Subm, and subreddits: {c}')\n",
    "            c += 1\n",
    "    \n",
    "    if(coms_only):\n",
    "        df_before_c = make_before_vals_df(sub_red_1, sub_red_2, submission_comment[0], b4[0], b4[1])   # FOR COMS ONLY\n",
    "        df_before_c.to_csv('./before_coms.csv', index=False)\n",
    "    else:\n",
    "        df_before_c = make_before_vals_df(sub_red_1, sub_red_2, submission_comment[0], b4[0], b4[1])   # FOR SUB AND COMS\n",
    "        df_before_c.to_csv('./before_coms.csv', index=False)\n",
    "        df_before_s = make_before_vals_df(sub_red_1, sub_red_2, submission_comment[1], b4[2], b4[3])\n",
    "        df_before_s.to_csv('./before_subm.csv', index=False)\n",
    "        \n",
    "    if(debug):\n",
    "        log_to_file(f'Length of list_df: {len(list_df)}')\n",
    "    return list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "trying-pantyhose",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_to_file(log_msg, log_file_name='', print_to_terminal=True, apply_date=True, show_info_tag = 1):\n",
    "    \"\"\"\n",
    "    Simple way to write to log.\n",
    "    Put in log message, and by default writes to '%Y-%m-%d_LOG.txt'\n",
    "    can change filename if you like with param 'log_file_name'\n",
    "    Also prints 'log_msg' to terminal by default with \n",
    "    param 'print_to_terminal=True'\n",
    "    Returns 'log_msg'\n",
    "    \"\"\"\n",
    "    info_tag = [ '', '[INFO] ','[WARN] ','[ERROR] ' ]\n",
    "    \n",
    "    if(len(log_file_name) == 0):\n",
    "        log_file_name = datetime.datetime.now().strftime('%Y-%m-%d_LOG.txt')\n",
    "\n",
    "    if(not os.path.exists(log_file_name)):\n",
    "        log_file = open(log_file_name, \"x\") # create file\n",
    "    \n",
    "    log_file = open(log_file_name, 'a') # appends msg to log\n",
    "    if(apply_date):\n",
    "        log_file.write(get_date_time())\n",
    "    if(show_info_tag > 0):\n",
    "        log_file.write(info_tag[show_info_tag])\n",
    "    log_file.write(log_msg)\n",
    "    log_file.write('\\n')\n",
    "    log_file.close()\n",
    "    if (print_to_terminal):\n",
    "        print(log_msg)\n",
    "    return log_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intense-inventory",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_special_line_to_log(line_type=0, log_file_name='', print_to_terminal=True, apply_date=False, show_info_tag = 0):\n",
    "    \"\"\"\n",
    "    Uses 'log_to_file' Function\n",
    "    Special commands to write to Log (and terminal if you want, but not by default):\n",
    "        0 = newline in log\n",
    "        1 = Long single line\n",
    "        2 = Long double line\n",
    "        3 = Short single line\n",
    "        4 = Short double line\n",
    "        5 = Long Star Line\n",
    "        6 = Short Star Line\n",
    "    \"\"\"\n",
    "    log_msg = [\n",
    "        '',\n",
    "        '--------------------------------------------------------------------------------------------------',\n",
    "        '==================================================================================================',\n",
    "        '--------------------------------------------------------------------',\n",
    "        '====================================================================',\n",
    "        '**************************************************************************************************',\n",
    "        '********************************************************************'\n",
    "    ]\n",
    "    \n",
    "    return log_to_file(log_msg[line_type], log_file_name, print_to_terminal, apply_date, show_info_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recent-notion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date_time():\n",
    "    \"\"\" Returns the date as a formatted string for logs: '%Y-%m-%d, %H:%M:%S - ' \"\"\"\n",
    "    return datetime.datetime.now().strftime('%Y-%m-%d, %H:%M:%S ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "turned-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_before_csv(filename):\n",
    "    \"\"\"\n",
    "    Read in CSV with Before Value and returns the 2 values in an array\n",
    "    Function returns array of 2 zeros if no file is found\n",
    "    \"\"\"\n",
    "    print(datetime.datetime.now().strftime('\\n\\nSTART:  %Y-%m-%d, %H:%M'))\n",
    "    try:\n",
    "        b4v = pd.read_csv(filename)\n",
    "        b4v_s = b4v['before_value'].str.replace('_', '').apply(int)\n",
    "        log_to_file('READ IN \"BEFORE\" VALS: ')\n",
    "        log_to_file(str(b4v_s), show_info_tag=0)\n",
    "    except:\n",
    "        log_to_file(' *** FILE NOT FOUND: {filename} ***')\n",
    "        log_to_file('      - SETTING \"BEFORE\" VALUES TO ZERO (0)')\n",
    "        return [0, 0]\n",
    "    return b4v_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "boring-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_scraping(url, sub_reddit, sub_com, size=100, before=0, num_retry=10):\n",
    "    \"\"\"\n",
    "    Calls Function 'set_params_and_request' and prints/logs status\n",
    "    If there is a 500 error, function can retry; Default is 10 times\n",
    "    requests data is converted to json and put into a Data Frame\n",
    "    Returns Data Frame of requests data\n",
    "    \"\"\"\n",
    "    time.sleep(10)\n",
    "    req = set_params_and_request(url, sub_reddit, size, before)\n",
    "    status = req.status_code\n",
    "    \n",
    "    log_tag = np.where(status == 200, 1, 2)\n",
    "    log_to_file(f'{sub_reddit} STATUS ({sub_com}): {status}', show_info_tag=log_tag)\n",
    "    \n",
    "    count = 0\n",
    "    while((status >= 500) & (count < num_retry)):\n",
    "        log_to_file('CONNECTION TIMED OUT', show_info_tag=2)\n",
    "        time.sleep(10)\n",
    "        req = set_params_and_request(url, sub_reddit, size, before)\n",
    "        status = req.status_code\n",
    "        log_to_file(f'{sub_reddit} STATUS ({sub_com}): {status}')\n",
    "        if(count == num_retry -1):\n",
    "            log_to_file('COULD NOT RESOLVE CONNECTION!', show_info_tag=3)\n",
    "        count += 1\n",
    "    \n",
    "    data = req.json()['data']\n",
    "    df_req = pd.DataFrame(data)\n",
    "    #write_to_csv(df_req, sub_reddit, sub_com)   # Old way\n",
    "    \n",
    "    return df_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "shaped-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params_and_request(url, sr, size, before, get_metadata = 'true'):\n",
    "    \"\"\"\n",
    "    Function sets up 'params' for pushshift.io reddit API and\n",
    "    Returns requests call: 'requests.get(url, params)'\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "\n",
    "    if(before <=0):\n",
    "        params = {\n",
    "            'subreddit': sr,\n",
    "            'size': size,\n",
    "            'metadata': get_metadata\n",
    "        }\n",
    "    else:\n",
    "        params = {\n",
    "            'subreddit': sr,\n",
    "            'size': size,\n",
    "            'before': before,    # 'created_utc'\n",
    "            'metadata': get_metadata\n",
    "        }\n",
    "    return requests.get(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "solved-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_before_vals_df(sr1, sr2, sub_com, value_sr1, value_sr2, to_terminal=True):\n",
    "    \"\"\"\n",
    "    'Before' values put into DataFrames and exported to CSV.\n",
    "    \"\"\"\n",
    "    before_list = [[sr1, sub_com, value_sr1],\n",
    "                   [sr2, sub_com, value_sr2]]\n",
    "    before = pd.DataFrame(before_list, columns=['subreddit', 'sub_com', 'before_value'])\n",
    "    before['before_value'] = '_' + before['before_value'].apply(str)\n",
    "    \n",
    "    log_to_file(f'BEFORE VAL, {sr1} {sub_com}: {value_sr1}', print_to_terminal=to_terminal)\n",
    "    log_to_file(f'BEFORE VAL, {sr2} {sub_com}: {value_sr2}', print_to_terminal=to_terminal)\n",
    "    print(datetime.datetime.now().strftime('FINISH: %Y-%m-%d, %H:%M\\n'))\n",
    "    return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "invisible-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(df_output, sr, sub_com, fn = ''):\n",
    "    \"\"\"\n",
    "    Writes a DataFrame to a csv file. Takes args for \n",
    "    'sr' (subreddit) and for 'sub_com' (whether 'submission' \n",
    "    or 'comment') and 'fn (filename)' - if: fn == '', filename \n",
    "    is given default, e.g. '2021-04-24_ama_comments.csv'. \n",
    "    Output is designated to '../data/' and cannot be altered. \n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../data/'):\n",
    "        log_to_file(f'  - Create Data Directory: ../data/')\n",
    "        os.makedirs('../data/')\n",
    "    if(len(fn) == 0):\n",
    "        fn = datetime.datetime.now().strftime('%Y-%m-%d_%H%M_') + sr + '_' + sub_com + '.csv'\n",
    "    output_file = '../data/' + fn\n",
    "    log_to_file('  - Writing to file: {fn}')\n",
    "    df_output.to_csv(output_file, index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educational-singles",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loop_call_scraper(num_loops, size=100, sub_reddit_1 = 'AMA', sub_reddit_2 = 'AskReddit', comments_only = False):\n",
    "    \"\"\"\n",
    "    Wrapper to the Function 'call_scraper' where it loops that function.\n",
    "    But it also grabs the returned list of DataFrames and compiles them \n",
    "    into four (or two) separate files per each subreddit and each \n",
    "    type (comments and if you want it, submissions - with \n",
    "    submissions is the default)\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    ersion = '1.1.0.1'\n",
    "    \n",
    "    list_df_sr1_comments = []\n",
    "    list_df_sr2_comments = []\n",
    "    list_df_sr1_submissions = []\n",
    "    list_df_sr2_submissions = []\n",
    "    \n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(2)\n",
    "    log_to_file(f'  BEGIN LOOP SCRAPER - v{ersion}')\n",
    "    print_special_line_to_log(1)\n",
    "    \n",
    "    for i in range(num_loops):\n",
    "        log_to_file(f'(lOOP # {i + 1} of {num_loops})')\n",
    "        ldf = call_scraper(size, sub_reddit_1, sub_reddit_2, coms_only=comments_only)\n",
    "        if(comments_only):\n",
    "            list_df_sr1_comments.append(ldf[0][0])\n",
    "            list_df_sr2_comments.append(ldf[1][0])\n",
    "        else:\n",
    "            list_df_sr1_comments.append(ldf[0][0])\n",
    "            list_df_sr2_comments.append(ldf[1][0])\n",
    "            list_df_sr1_submissions.append(ldf[2][0])\n",
    "            list_df_sr2_submissions.append(ldf[3][0])\n",
    "            \n",
    "    log_to_file(f' *** Writing to CSV file(s) ***')\n",
    "    if(comments_only):\n",
    "        big_ol_sr1_comm_df = pd.concat(list_df_sr1_comments)\n",
    "        big_ol_sr2_comm_df = pd.concat(list_df_sr2_comments)\n",
    "        write_to_csv(big_ol_sr1_comm_df, sub_reddit_1, 'comments')\n",
    "        write_to_csv(big_ol_sr2_comm_df, sub_reddit_2, 'comments')\n",
    "    else:\n",
    "        big_ol_sr1_comm_df = pd.concat(list_df_sr1_comments)\n",
    "        big_ol_sr2_comm_df = pd.concat(list_df_sr2_comments)\n",
    "        big_ol_sr1_subm_df = pd.concat(list_df_sr1_submissions)\n",
    "        big_ol_sr2_subm_df = pd.concat(list_df_sr2_submissions)\n",
    "        write_to_csv(big_ol_sr1_comm_df, sub_reddit_1, 'comments')\n",
    "        write_to_csv(big_ol_sr2_comm_df, sub_reddit_2, 'comments')\n",
    "        write_to_csv(big_ol_sr1_subm_df, sub_reddit_1, 'submissions')\n",
    "        write_to_csv(big_ol_sr2_subm_df, sub_reddit_2, 'submissions')\n",
    "    \n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(1)\n",
    "    log_to_file(f'  FINISH LOOP SCRAPER')\n",
    "    print_special_line_to_log(2)\n",
    "    print_special_line_to_log(0)\n",
    "    print_special_line_to_log(0)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quiet-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################\n",
    "###### CALL THE MAIN FUNCTION!!! ######\n",
    "#######################################\n",
    "\n",
    "\n",
    "#loop_call_scraper(10, size=200, sub_reddit_1 = 'AMA', sub_reddit_2 = 'AskReddit', comments_only = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "manual-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================================================================\n",
      "  BEGIN LOOP SCRAPER - v1.1.0.1\n",
      "--------------------------------------------------------------------------------------------------\n",
      "(lOOP # 1 of 2)\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "  Begin Iteration of call_scraper\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "\n",
      "START:  2021-05-02, 20:47\n",
      "READ IN \"BEFORE\" VALS: \n",
      "0    1619521176\n",
      "1    1619548820\n",
      "Name: before_value, dtype: int64\n",
      "AMA STATUS (comment): 200\n",
      "AskReddit STATUS (comment): 200\n",
      "BEFORE VAL, AMA comment: 1619520758\n",
      "BEFORE VAL, AskReddit comment: 1619548819\n",
      "FINISH: 2021-05-02, 20:47\n",
      "\n",
      "(lOOP # 2 of 2)\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "  Begin Iteration of call_scraper\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "\n",
      "START:  2021-05-02, 20:47\n",
      "READ IN \"BEFORE\" VALS: \n",
      "0    1619520758\n",
      "1    1619548819\n",
      "Name: before_value, dtype: int64\n",
      "AMA STATUS (comment): 200\n",
      "AskReddit STATUS (comment): 200\n",
      "BEFORE VAL, AMA comment: 1619520025\n",
      "BEFORE VAL, AskReddit comment: 1619548816\n",
      "FINISH: 2021-05-02, 20:48\n",
      "\n",
      " *** Writing to CSV file(s) ***\n",
      "  - Writing to file: {fn}\n",
      "  - Writing to file: {fn}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------\n",
      "  FINISH LOOP SCRAPER\n",
      "==================================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loop_call_scraper(20, size=100, sub_reddit_1 = 'AMA', sub_reddit_2 = 'AskReddit', comments_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-server",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
